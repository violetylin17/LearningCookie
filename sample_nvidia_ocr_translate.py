# -*- coding: utf-8 -*-
"""sample_NVIDIA_OCR_translate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lW9uTG-9_ZljJCWPk1uDRQmNhB-X0KHQ
"""

!pip install openai

from openai import OpenAI

# Set up the NVIDIA NIM client
client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key="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"  # Enter your issued API key here
)
# List available models
models = client.models.list()
for model in models:
    print(model.id)

"""I will use nvidia/nvidia-nemotron-nano-9b-v2, nvidia/llama-3.3-nemotron-super-49b-v1.5

Simple text OpenAI
"""

from openai import OpenAI

client = OpenAI(
  base_url = "https://integrate.api.nvidia.com/v1",
  api_key = "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"
)

completion = client.chat.completions.create(
  model="nvidia/llama-3.3-nemotron-super-49b-v1.5",
  messages=[{"role":"user","content":""}],
  temperature=0.6,
  top_p=0.95,
  max_tokens=65536,
  frequency_penalty=0,
  presence_penalty=0,
  stream=False
)

print(completion.choices[0].message)

pip install pytesseract

# Install Tesseract OCR and language packs
!apt-get install -y tesseract-ocr
!apt-get install -y tesseract-ocr-kor  # Korean language pack
!apt-get install -y tesseract-ocr-eng  # English (backup)
!apt-get install -y tesseract-ocr-vie # vietnamese

# Verify installation
!tesseract --list-langs



# --- Colab OCR + optional translation (NVIDIA Nemotron) ---
from google.colab import files
from IPython.display import display, Image as IPImage  # Renamed to avoid PIL name conflict
from PIL import Image, ImageOps, ImageDraw, ImageFont
import numpy as np, cv2, pytesseract, os, io
from openai import OpenAI

# ===== CONFIGURATION =====
TARGET_LANG = "English"    # Target translation language
OCR_LANG    = "kor"        # Tesseract language
OUTPUT_IMAGE = "translated_output.png"

# NVIDIA Nemotron settings
USE_TRANSLATION = True
NVIDIA_API_KEY = "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"
NVIDIA_BASE_URL = "https://integrate.api.nvidia.com/v1"
NVIDIA_MODEL = "nvidia/llama-3.3-nemotron-super-49b-v1.5"

client = OpenAI(base_url=NVIDIA_BASE_URL, api_key=NVIDIA_API_KEY)

def preprocess_variants(pil_img):
    """Generate multiple preprocessed versions of the image for better OCR detection."""
    im = pil_img.convert("RGB")
    arr = np.array(im)
    big = cv2.resize(arr, None, fx=2.0, fy=2.0, interpolation=cv2.INTER_CUBIC)

    gray = cv2.cvtColor(big, cv2.COLOR_RGB2GRAY)
    gray = cv2.convertScaleAbs(gray, alpha=1.3, beta=10)
    den = cv2.bilateralFilter(gray, 9, 75, 75)

    _, otsu = cv2.threshold(den, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    adapt = cv2.adaptiveThreshold(den, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                  cv2.THRESH_BINARY, 31, 9)

    kernel = np.ones((2,2), np.uint8)
    dil = cv2.dilate(otsu, kernel, iterations=1)
    er  = cv2.erode(otsu, kernel, iterations=1)

    return [
        ("raw_upscaled_gray", gray),
        ("otsu", otsu),
        ("adaptive", adapt),
        ("dilated", dil),
        ("eroded", er),
    ]

def ocr_multi_try(pil_img, lang="eng"):
    """Run multiple OCR passes with different preprocessing + page segmentation modes."""
    variants = preprocess_variants(pil_img)
    psms = [6, 11, 3, 4, 12]
    best = {"text":"", "score":0, "note":""}

    for name, img in variants:
        for psm in psms:
            cfg = f"--oem 3 --psm {psm}"
            txt = pytesseract.image_to_string(img, lang=lang, config=cfg)
            raw = txt.strip()
            if not raw:
                continue
            alnum = sum(ch.isalnum() for ch in raw)
            score = alnum + len(raw)
            if score > best["score"]:
                best = {"text":raw, "score":score, "note":f"{name}|psm={psm}"}
    return best

def translate_text_nemotron(text, target="English"):
    """Translate text using NVIDIA Nemotron."""
    try:
        resp = client.chat.completions.create(
            model=NVIDIA_MODEL,
            messages=[{
                "role":"user",
                "content": f"Translate the following text to {target}. Only output the translation without any explanation:{text}"
            }],
            temperature=0.2,
            max_tokens=1000
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        return None, str(e)

def center_text_on_image(base_size, text, font_path=None, font_size=36, margin=30):
    """Render centered text on a white image."""
    w, h = base_size
    new_img = Image.new("RGB", (w, h), "white")
    draw = ImageDraw.Draw(new_img)
    try:
        font = ImageFont.truetype(font_path or "DejaVuSans.ttf", font_size)
    except:
        font = ImageFont.load_default()

    def wrap(draw_obj, t, fnt, max_w):
        words = t.split()
        lines, cur = [], ""
        for w_ in words:
            trial = (cur + " " + w_).strip()
            bb = draw_obj.textbbox((0,0), trial, font=fnt)
            if (bb[2]-bb[0]) <= max_w:
                cur = trial
            else:
                if cur:
                    lines.append(cur)
                cur = w_
        if cur:
            lines.append(cur)
        return "\n".join(lines)

    wrapped = wrap(draw, text, font, w - 2*margin)
    bbox = draw.multiline_textbbox((0,0), wrapped, font=font, spacing=8, align="center")
    tw, th = bbox[2]-bbox[0], bbox[3]-bbox[1]
    x, y = (w - tw)//2, (h - th)//2
    draw.multiline_text((x, y), wrapped, fill="black", font=font, align="center", spacing=8)
    return new_img

# --- MAIN EXECUTION (Colab) ---
uploaded = files.upload()
image_path = next(iter(uploaded))
orig = Image.open(image_path)

print("ðŸ”Ž Running OCR (multi-pass)...")
best = ocr_multi_try(orig, lang=OCR_LANG)
ocr_text = best["text"]
print(f"â€¢ Variant/PSM: {best['note']}")
print("â€¢ OCR Result:")
print(ocr_text if ocr_text else "(No text detected)")

translated = None
err_msg = None
if USE_TRANSLATION and ocr_text:
    print("ðŸŒ Translating...")
    out = translate_text_nemotron(ocr_text, target=TARGET_LANG)
    if isinstance(out, tuple):
        translated, err_msg = out
    else:
        translated = out

if translated is None and err_msg:
    print(f"âš ï¸ Translation skipped (API error): {err_msg}")

final_text = translated if translated else ocr_text or "(No text detected)"
print("âœ… Final Text to Render:")
print(final_text)

out_img = center_text_on_image(orig.size, final_text, font_size=36)
out_img.save(OUTPUT_IMAGE)
display(IPImage(filename=OUTPUT_IMAGE))
files.download(OUTPUT_IMAGE)